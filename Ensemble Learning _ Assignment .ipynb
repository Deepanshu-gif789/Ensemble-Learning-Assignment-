{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPPsZaIjlYh/7zu1SzjQe2A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Ensemble Learning | Assignment"],"metadata":{"id":"7taJtJ0z3LSb"}},{"cell_type":"markdown","source":["1. **What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n","     - Ensemble Learning is a machine learning technique that combines the predictions of multiple models to produce a more accurate and robust final result than any single model could achieve. The key idea behind ensemble learning is that a group of weak learners, when combined properly, can perform better than an individual strong learner.\n","In other words, ensemble methods leverage the diversity among models — by averaging, voting, or stacking their outputs — to reduce errors caused by bias, variance, or noise in the data.\n","\n","     - Common ensemble techniques include:\n","\n","        - **Bagging (Bootstrap Aggregating)**: Trains multiple models on random subsets of data and combines their results (e.g., Random Forest).\n","\n","        - **Boosting**: Sequentially builds models that focus on correcting the errors of previous ones (e.g., AdaBoost, Gradient Boosting).\n","\n","        - **Stacking**: Combines predictions from multiple models using another model (meta-learner) to make the final decision."],"metadata":{"id":"IsBZLVKM3RLN"}},{"cell_type":"markdown","source":["2.  **What is the difference between Bagging and Boosting?**\n","\n","     - Ensemble learning is a technique that combines multiple machine learning models to improve overall performance. Two popular ensemble methods are Bagging and Boosting, and the main difference between them lies in how these models are trained and combined.\n","        - **Bagging (Bootstrap Aggregating)**: In Bagging, several models (like Decision Trees) are trained independently on different random subsets of the training data. The final prediction is made by averaging (for regression) or voting (for classification) the results of all models. Bagging helps to reduce variance and prevent overfitting.\n","        Example: Random Forest.  \n","        - **Boosting**: In Boosting, models are trained sequentially, where each new model focuses on correcting the errors made by the previous ones. It assigns higher weights to misclassified data points so that the next model learns them better. Boosting helps to reduce bias and improve accuracy.\n","        Example: AdaBoost, Gradient Boosting, XGBoost."],"metadata":{"id":"rq0JTEye4MR7"}},{"cell_type":"markdown","source":["3.  **What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n","      - Bootstrap sampling is a statistical technique used to create multiple random samples from the original dataset with replacement. This means some data points may appear multiple times in a sample, while others may not appear at all.\n","\n","           In Bagging methods like Random Forest, bootstrap sampling plays a key role by ensuring that each base model (e.g., each decision tree) is trained on a slightly different version of the data. This diversity among the models helps to:\n","\n","           Reduce overfitting by preventing all models from seeing the same data.\n","           Increase model stability and accuracy when their predictions are combined through averaging or majority voting."],"metadata":{"id":"q8Y8Ccf6474v"}},{"cell_type":"markdown","source":["4.  **What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**\n","      - Out-of-Bag (OOB) samples are the data points that are not included in a particular bootstrap sample during the training of ensemble models like Random Forest. Since each tree in the forest is trained on a random subset of the data (with replacement), about one-third of the original data is left out as OOB samples for that tree. The OOB score is a performance estimate calculated by testing each tree on its corresponding OOB samples — the data it didn’t see during training. The predictions from all trees for their OOB samples are then combined to compute an overall accuracy or error rate."],"metadata":{"id":"DlZlpeww5jAF"}},{"cell_type":"markdown","source":["5.  **Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n","      - In a single Decision Tree, feature importance is calculated based on how much each feature reduces impurity (like Gini impurity or entropy) across all its splits. Features used in higher-level splits or that produce greater impurity reduction are considered more important. However, since a single tree can be sensitive to small changes in the data, its feature importance may not be very reliable. In contrast, a Random Forest calculates feature importance by averaging the importance scores of each feature across all trees in the ensemble. Because Random Forest combines many trees trained on different subsets of data and features, its importance values are more stable, robust, and generalizable compared to a single Decision Tree."],"metadata":{"id":"yhUXtZJj5zJ1"}},{"cell_type":"markdown","source":["6. Write a Python program to:\n","     - Load the Breast Cancer dataset using\n","        **sklearn.datasets.load_breast_cancer()**\n","     -  Train a Random Forest Classifier\n","     -  Print the top 5 most important features based on feature importance scores."],"metadata":{"id":"eYE-O0396LVx"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ck63pU663AlD","executionInfo":{"status":"ok","timestamp":1761841204337,"user_tz":-330,"elapsed":7064,"user":{"displayName":"Deepanshu Aggrawal","userId":"13570431902424540254"}},"outputId":"48539ad7-feb9-4e60-ec1b-ed7065af983a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Top 5 Most Important Features:\n","worst area              0.153892\n","worst concave points    0.144663\n","mean concave points     0.106210\n","worst radius            0.077987\n","mean concavity          0.068001\n","dtype: float64\n"]}],"source":["# Import necessary libraries\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","# Load the Breast Cancer dataset\n","data = load_breast_cancer()\n","X = pd.DataFrame(data.data, columns=data.feature_names)\n","y = data.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a Random Forest Classifier\n","model = RandomForestClassifier(random_state=42)\n","model.fit(X_train, y_train)\n","\n","# Get feature importances\n","feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n","\n","# Print the top 5 most important features\n","top_features = feature_importances.sort_values(ascending=False).head(5)\n","print(\"Top 5 Most Important Features:\")\n","print(top_features)\n"]},{"cell_type":"markdown","source":["7.  Write a Python program to:\n","     - Train a Bagging Classifier using\n","        \n","        **Decision Trees on the Iris dataset**\n","     - Evaluate its accuracy and compare with a single Decision Tree"],"metadata":{"id":"Xal5v01560Bl"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a single Decision Tree\n","dt_model = DecisionTreeClassifier(random_state=42)\n","dt_model.fit(X_train, y_train)\n","dt_pred = dt_model.predict(X_test)\n","dt_accuracy = accuracy_score(y_test, dt_pred)\n","\n","# Train a Bagging Classifier using Decision Trees as base estimators\n","bag_model = BaggingClassifier(\n","    estimator=DecisionTreeClassifier(),\n","    n_estimators=50,\n","    random_state=42\n",")\n","bag_model.fit(X_train, y_train)\n","bag_pred = bag_model.predict(X_test)\n","bag_accuracy = accuracy_score(y_test, bag_pred)\n","\n","# Compare accuracies\n","print(\"Accuracy of Single Decision Tree:\", round(dt_accuracy, 3))\n","print(\"Accuracy of Bagging Classifier:\", round(bag_accuracy, 3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h2WfRGWa3GDz","executionInfo":{"status":"ok","timestamp":1761841377696,"user_tz":-330,"elapsed":210,"user":{"displayName":"Deepanshu Aggrawal","userId":"13570431902424540254"}},"outputId":"1e22e1b6-43b6-46e5-d498-f119a04b606c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of Single Decision Tree: 1.0\n","Accuracy of Bagging Classifier: 1.0\n"]}]},{"cell_type":"markdown","source":["8.  Write a Python program to:\n","     - Train a Random Forest Classifier  \n","       **Tune hyperparameters max_depth and n_estimators using GridSearchCV**\n","     - Print the best parameters and final accuracy"],"metadata":{"id":"9yOoz8J17ju0"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Breast Cancer dataset\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Random Forest Classifier\n","rf = RandomForestClassifier(random_state=42)\n","\n","# Define the hyperparameter grid for tuning\n","param_grid = {\n","    'n_estimators': [50, 100, 150],\n","    'max_depth': [3, 5, 8, None]\n","}\n","\n","# Perform Grid Search with 5-fold cross-validation\n","grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n","                           cv=5, scoring='accuracy', n_jobs=-1)\n","\n","# Fit the model on training data\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best parameters and accuracy\n","best_params = grid_search.best_params_\n","best_model = grid_search.best_estimator_\n","\n","# Evaluate on test data\n","y_pred = best_model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print results\n","print(\"Best Parameters found by GridSearchCV:\", best_params)\n","print(\"Final Model Accuracy on Test Data:\", round(accuracy, 3))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WllrUot171Wx","executionInfo":{"status":"ok","timestamp":1761841521164,"user_tz":-330,"elapsed":20537,"user":{"displayName":"Deepanshu Aggrawal","userId":"13570431902424540254"}},"outputId":"fc15d4c8-5a06-4e82-e8b9-671dd1173963"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters found by GridSearchCV: {'max_depth': 8, 'n_estimators': 150}\n","Final Model Accuracy on Test Data: 0.965\n"]}]},{"cell_type":"markdown","source":["9.  Write a Python program to:\n","     - Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n","     - Compare their Mean Squared Errors (MSE)"],"metadata":{"id":"3To4wcds8GdN"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# Load the California Housing dataset\n","data = fetch_california_housing()\n","X, y = data.data, data.target\n","\n","# Split into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a Bagging Regressor using Decision Trees as base estimators\n","bagging_reg = BaggingRegressor(\n","    estimator=DecisionTreeRegressor(),\n","    n_estimators=100,\n","    random_state=42\n",")\n","bagging_reg.fit(X_train, y_train)\n","\n","# Train a Random Forest Regressor\n","rf_reg = RandomForestRegressor(\n","    n_estimators=100,\n","    random_state=42\n",")\n","rf_reg.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_bag = bagging_reg.predict(X_test)\n","y_pred_rf = rf_reg.predict(X_test)\n","\n","# Calculate Mean Squared Error (MSE)\n","mse_bag = mean_squared_error(y_test, y_pred_bag)\n","mse_rf = mean_squared_error(y_test, y_pred_rf)\n","\n","# Print the results\n","print(\"Mean Squared Error (Bagging Regressor):\", round(mse_bag, 4))\n","print(\"Mean Squared Error (Random Forest Regressor):\", round(mse_rf, 4))\n","\n","# Compare performance\n","if mse_rf < mse_bag:\n","    print(\"\\n✅ Random Forest performs better (lower MSE).\")\n","else:\n","    print(\"\\n✅ Bagging Regressor performs better (lower MSE).\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DT4Lq-kN7VUm","executionInfo":{"status":"ok","timestamp":1761841673537,"user_tz":-330,"elapsed":44351,"user":{"displayName":"Deepanshu Aggrawal","userId":"13570431902424540254"}},"outputId":"c9076bc1-e7fc-4604-bee9-b4b0f6d51eef"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error (Bagging Regressor): 0.2559\n","Mean Squared Error (Random Forest Regressor): 0.2554\n","\n","✅ Random Forest performs better (lower MSE).\n"]}]},{"cell_type":"markdown","source":["10.  You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n","   - Choose between Bagging or Boosting\n","   - Handle overfitting\n","   - Select base models\n","   - Evaluate performance using cross-validation\n","   - Justify how ensemble learning improves decision-making in this real-world context."],"metadata":{"id":"umM56Ate8sLM"}},{"cell_type":"markdown","source":["**ANSWER** - Loan Default Prediction using Ensemble Learning\n","\n","  As a data scientist in a financial institution, my goal is to predict loan default accurately using customer demographic and transaction history data. To improve performance and reliability, I would apply ensemble learning techniques.\n","\n","  1. **Choosing between Bagging and Boosting:** - Bagging methods like Random Forest reduce variance and are effective when models tend to overfit. Boosting methods like XGBoost or LightGBM reduce bias by focusing on difficult samples. For this task, Boosting is preferred because it performs better on complex tabular data and can capture subtle patterns in customer behavior.\n","\n","  2. **Handling Overfitting:** - I would use techniques like cross-validation, early stopping, regularization parameters (e.g., learning rate, max_depth), and feature selection. Additionally, tuning hyperparameters carefully and using class weights for imbalanced data helps control overfitting.\n","\n","  3. **Selecting Base Models:** - The base models could include Decision Trees for simplicity or Logistic Regression for interpretability. For ensembles, I would use Random Forest (Bagging) and XGBoost/LightGBM (Boosting) as they are robust and widely used in financial modeling.\n","\n","  4. **Evaluating Performance:** - I would use Stratified K-Fold Cross-Validation to ensure balanced class representation. The main evaluation metrics would be AUC-ROC, Precision, Recall, and F1-score. These metrics help assess the model’s ability to distinguish between defaulters and non-defaulters effectively.\n","\n","  5. **Justification of Ensemble Learning:** - Ensemble methods combine multiple weak models to form a stronger one, improving prediction accuracy and stability. They reduce both variance and bias, leading to more reliable credit risk assessment. In a real-world context, this means fewer false approvals, reduced financial losses, and better decision-making for loan approvals."],"metadata":{"id":"lzA__38E-Qxd"}},{"cell_type":"code","source":[],"metadata":{"id":"TDjvdRxX8YQd"},"execution_count":null,"outputs":[]}]}